{
    "contents" : "---\ntitle: 'Linear Regression and Correlation'\nauthor: \"DS705\"\nfontsize: '12pt'\noutput:\n  beamer_presentation:\n    template: '../beamer169experimental.tex'\n    colortheme: \"seahorse\"\n    keep_tex: true\n    fonttheme: default\n---\n\n```{r global_options, include=FALSE, echo=FALSE}\n# use for beamer\nknitr::opts_chunk$set(fig.width=4, fig.height=3, fig.align='center',warning=FALSE, message=FALSE)\nlibrary(knitr)\nlibrary(car)\n# use for word\n# knitr::opts_chunk$set(fig.width=4, fig.height=3,warning=FALSE, message=FALSE)\n\n# save par defaults\n.pardefaults <- par()\n\n# Ott 11.45\ndose <- rep(c(2,4,8,16,32,64),c(3,2,2,3,2,3))\nstrength <- c(5,7,3,10,14,15,17,20,21,19,23,29,28,31,30)\ndrug <- data.frame(dose,strength)\n\n# Ott 11.12\nspeed <- rep(c(60,80,100,120,140),each=4)\nlifetime <- .1*c(46,38,49,45,47,58,55,54,50,45,32,48,41,45,40,38,36,30,35,34) \ndrills <- data.frame(speed,lifetime)\n\n# example is from these lecture notes\n# http://scc.stat.ucla.edu/page_attachments/0000/0139/reg_1.pdf\n#\n# data is from Simon Sheather book \"A Modern Approach to Regression with R\"\n#production <- read.table(\"http://www.stat.tamu.edu/~sheather/book/docs#/datasets/production.txt\", header=T, sep=\"\")\nproduction <- read.table('./production.txt',header=T,sep=\"\")\nproduction <- production[,c(3,2)]\nnames(production)<-c(\"NumItems\",\"Time\")\n```\n\n## Relationships\n\n\\hfill \\includegraphics[width=5.5in]{./figures/Relationships_Table.pdf} \\hspace*{\\fill}\n\n<div class='notes'>\n- much of statistics is about exploring relationships between variables\n- In ANOVA for instance we might be interested in looking at the relationship between the type of curriculum taught and the reading score achieved by students.  \n    - the explanatory or independent variable $x$ is the type of curriculum\n    - the response or dependent variable $y$ is the reading score\n- This unit is about simple linear regression when there is a single quantitative explantory variable and a single quantitative respose variable\n- In future units, you'll learning about having multiple explanatory variables or even having multiple response variables\n</div>\n\n## Explore Relationships and Make Predictions\n\nExample: more items $\\Rightarrow$ more time\n\n- Model the relationship.\n- Make predictions.\n\n$$ x = \\mbox{ number of items}, \\hspace{.5in}  y = \\mbox{ production time (minutes)}$$\n\n## Sample Data\n\n```{r echo=TRUE}\nhead(production)\n```\n\nData from *Business Analysis Using Regression: A Casebook\" by Foster, Stine, and Waterman.\n\n<div class='notes'>\n- *could move the citation to the block below the slide*\n- production is a dataframe with two columns NumItems and Time\n- each row reprents a single observation in which both the number of times and the production time were recorded\n</div>\n<!------------------------------------------------------------------>\n\n## Plot the data\n\nAlways start with a scatterplot:\n\n```{r}\nwith(production,plot(NumItems,Time))\n```\n\n<div class='notes'>\n- is there a linear trend?\n- does the data look like a line with added noise?\n</div>\n<!------------------------------------------------------------------>\n\n## Correlation\n\nHow strong is the *linear* relation between $x$ and $y$?\n\n```{r}\nwith(production, cor.test( NumItems, Time)$estimate )\n```\n\nNear $+1 \\Rightarrow$ strong, positive linear relationship.\n\n<div class='notes'>\n- correlation is useful only for linear relationships.\n- we verified this was a linear relationship by inspecting the scatter plot.\n- cor.test produces more than simply the correlation coeficient, we'll look at some of the other stuff later.\n</div>\n<!------------------------------------------------------------------>\n\n## Desired Model\n\n\\Large \n\n$$\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x$$\n\n- $x =$ number of items\n- $y =$ production time in muinutes\n- $\\hat{y}$ prediced value of $y$\n- $\\beta_{0}$ estimated $y$-intercept\n- $\\beta_{1}$ estimated slope of line\n\n<div class='notes'>\n\n</div>\n<!------------------------------------------------------------------>\n\n## Residuals\n\n\\vspace{-.3in}\n\n```{r echo=FALSE, fig.height=2.5,fig.width=5}\nlinear.model <- lm(Time~NumItems,data=production)\nfits <- linear.model$fitted\npar(mar=c(4,4,.25,.25))\nwith(production,plot(NumItems,Time))\nabline(linear.model)\nsegments(x0=production$NumItems,y0=fits,x1=production$NumItems,y1=production$Time,lty=\"dotted\")\n````\n\nresidual $=e_i = \\hat{y}_i - y_i$\n\n<div class='notes'>\n- finding the line is all about the residuals\n- a residual is the vertical difference between an observed y-value and the predicted y-value from the line\n- the least squares regression line is found by choosing the line that minimizes the sum square residuals\n- we'll see how to find the model and plot the line in the next few slides\n</div>\n<!------------------------------------------------------------------>\n\n## Finding the model in R\n\n```{r eval = FALSE}\nlinear.model <- with( production, lm( Time ~ NumItems ) )\nsummary(linear.model)\n```\n\n```{r, echo=FALSE,warning=FALSE}\n# this is just tricks to trim the output to fit on one slide\nlinear.model <- with( production, lm( Time ~ NumItems ) )\nrequire(utils)\ntmp <- noquote( \n  capture.output( \n    summary(linear.model)\n    )\n  )\nwrite.table(tmp[9:12],quote=F,row.names=F,col.names=F)\n```\n\n$$\\hat{y} = 149.75 + 0.2592 x$$\n$$\\mbox{Time } = 149.75 + 0.2592\\mbox{ NumItems}$$\n\n<div class='notes'>\n- summary actually reports a whole lot more than the coefficients of the model and we'll look at more of it later, for now we are just interested in the coefficients of the model (Higlight the block\n(Intercept) 149.74770 and NumItems 0.25924)\n- \n</div>\n<!------------------------------------------------------------------>\n\n## Plotting the least-squares line\n\n```{r}\nwith( production, plot( NumItems, Time) )\nabline( linear.model, col = 'blue' )\n```\n\n\n<div class='notes'>\n\n</div>\n<!------------------------------------------------------------------>\n\n## Extracting Coefficients\n\n- Type `str(mod)` to see everything\n\n```{r}\nlinear.model$coef[1]\nlinear.model$coef[2]\n```\n\n*Average* production time increases 0.26 minutes for each additional item produced.\n\n\n<div class='notes'>\n- the structure command, str,  is always useful as functions produces all sorts of\ndifferent objects\n</div>\n<!------------------------------------------------------------------>\n\n## Making Predictions (2)\n\n\n```{r}\nnew <- data.frame( NumItems = seq(50,350,by=50) )\nnew$Time <- predict.lm( linear.model, new )\nnew\n```\n\n<div class='notes'>\n- the predict.lm method requires that the x-values be passed as a dataframe\ncontaining a column with the same name as the predictor variable\n</div> \n<!------------------------------------------------------------------>\n\n## Inference for Regression\n\n\\large  \n\n- estimate population slope\n- test for signficant linear relationship / correlation\n- estimate average response at given $x$\n- estimate future individual response at given $x$\n\n<div class='notes'>\n- These are some of the typical things that are done with a linear regression model\n- but certain requirements must be met before we can make statistical inferences\n- to understand these requirements, you should know a little about the linear regression model\n</div>\n<!------------------------------------------------------------------>\n\n## Simple Linear Regression Model\n\nSimple Linear Regression: \n$$y_i  = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\hspace{.5in}\n\\epsilon_i \\overset{\\scriptsize\\mbox{ind}}{\\sim} N(0,\\sigma_\\epsilon)$$\n$$ x = \\mbox{ explanatory, independent, predictor }, y = \\mbox{ response, depdendent }$$\n\nAssumptions / Requirments:\n\n1. errors have mean 0\n2. errors have the same variance for all $x$\n3. errors are independent of each other\n4. errors are normally distributed.\n\n<div class='notes'>\n- of course you can fit a line to any data you like, but if you want to make inferences based on the linear relationship, you need need to first verify that these requirements are met.  \n- if the requirements are not met, then more advanced techniques, beyond those discussed below are needed.\n</div>\n\n\n## Errors vs. Residuals\n\n- Errors are differences between the true, but unknown, line and the $y$ values ($\\epsilon$ in the model)\n- Residuals are the differences between the estimated line and the $y$ values\n- The residuals approximate the errors.\n- Inspect the residuals to see if the model requirements are plausible.\n\n## Checking Requirements\n\nAssumptions / Requirments:\n\n1. errors have mean 0\n2. errors have the same variance for all $x$\n3. errors are independent of each other\n4. errors are normally distributed.\n\nTo make things simpler extract all the info. first:\n\n```{r}\nresids <- linear.model$resid # extract residuals from model\nNumItems <- production$NumItems\nTime <- production$Time\nTimeFit <- linear.model$fitted.values\n```\n\n<div class='notes'>\n- We are going to use the residuals as surrogates for the errors and by inspecting the residuals see if each requirement seems to be satisfied\n- The first requirment is automatic, since mathematically the residuals always add to zero.\n</div>\n<!------------------------------------------------------------------>\n\n## Equal Variances (homoscedasticity)\n\nDo the errors have the same variance for all $x$?  \n\n```{r}\nplot(NumItems,resids); abline(h=0,lty='dashed')\n```\n\n<div class='notes'>\n- we want to see the same amount of spread, vertically, at all values of $x$\n- this certainly seems to be true for this data\n- the next slide shows an example of the kind of residual plot we do not want to see\n</div>\n<!------------------------------------------------------------------>\n\n## Equal Variance (homescedasticity 2)\n\nEquivalently, we can plot the residuals versus the fitted values\n\n```{r}\nplot(TimeFit, resids); abline( h=0, lty='dashed')\n```\n\n<div class='notes'>\n- the last two plots look exactly the same and for simple linear regression they are identical\n- in multiple regression there are multiple explanatory variables $x$ so we look at only the plot if the residuals versus the fitted values\n</div>\n<!------------------------------------------------------------------->\n\n## Equal Variance (homoscedasticity 3)\n\nWhat we don't want to see:\n\n```{r echo = FALSE,fig.width=5,fig.height=2.5,fig.align='center'}\nind <- order(NumItems)\nxtmp <- NumItems[ind]\nw <- 1 +6*((0:19)/19)\nresidtmp <- resids*w\npar(mar=c(.25,.25,.25,.25))\nplot(xtmp,residtmp)\nabline(h=0,lty='dashed')\n```\n<div class='notes'>\n- the spread of the residuals increases as x increases suggesting that\nthe variance is not independent of $x$.  This is called heteroscedasticity.\n</div>\n\n## Testing for equal variances\n\nThe Bruesch-Pagan test.  A low $P$-value indicates unequal variances.\n$$ H_0: \\mbox{equal variances}, \\hspace{.5in} H_1: \\mbox{unequal variances}$$\n\n```{r}\nrequire(lmtest) # install if needed\nbptest(linear.model)\n```\n\n<div class='notes'>\nYou can read more about the Bruesh-Pagan test on page 800 of Ott's book.\n</div>\n\n## Independence of Errors\n\n```{r,echo=FALSE}\nplot( NumItems, resids ); abline( h=0, lty='dashed' )\n```\n\n<div class='notes'>\n- this can be hard to check by inspection.  it helps to know how the data was collected.\n- however, we can look at this residual plot to get an idea\n- if there is any sequential pattern to the residuals, then the errors are likely not indepenent, but not seeing a pattern does not prove they are independent, you may need to inspect the data in other ways\n- for instance, if there is a chronological order to the data and you can plot the residuals versus time, then a pattern would indicate dependence of the errors ...\n- we see no sequential pattern in this residual plot, so independence of errors is plausible \n</div>\n\n## Normality of Error Distribution\n\n```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}\npar(mar=c(4,4,.2,.25))\n```\n\n```{r,fig.width=5,fig.height=3,fig.align='center'}\npar(mfrow=c(1,2)); hist( resids); qqnorm( resids); qqline( resids)\n```\n\n\n<div class='notes'>\n- the histogram and normal quantile plot both suggest that the residuals\ncould be normally distributed so its plausible that the errors are normally distributed\n- having verified that are data seems to be the simple linear regression model, we can continue on to doing statistical inference for this problem\n</div>\n<!------------------------------------------------------------------>\n\n\n## Outliers and Influential Observations\n\n```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}\npar(mar=c(.25,.25,.25,.25),mfrow=c(1,1))\n```\n\n```{r}\nx <- c(2,7,7,8,11,13,15,33); y <- c(1,9,8,10,18,26,19,30)\nplot( x, y, ylim=c(0,50) )\nmod1 <- lm( y~x ); mod2 <- lm( y[-8] ~ x[-8] )\nabline(mod1); abline(mod2,lty='dashed')\n```\n\n<div class='notes'>\n- as always outliers are any observations that are outside the pattern, the point (33,30) is an outlier\n- an outlier doesn't have to be influential and an influential point need not be an outlier\n- to see if a point is influential, plot the line with all the data, this is the solid line\n- and plot the line with the point (33,30) removed resulting in the dashed line\n- since the line is much different with the point removed, the point (33,30 is an influential point\n- we should determine if we want to restrict our attention to x values between 2 and 15 in which case the dashed line is fine, or if we need to extend the model further then we probably need additional data before we can assess whether a linear model is appropriate\n</div>\n\n## Test for a significant linear relationship\n\n$$H_a: \\beta_1 \\neq 0$$\n\n```{r eval = FALSE}\nlinear.model <- with( production, lm( Time ~ NumItems ) )\nsummary(linear.model)\n```\n\n```{r, echo=FALSE,warning=FALSE}\n# this is just tricks to trim the output to fit on one slide\nlinear.model <- with( production, lm( Time ~ NumItems ) )\nrequire(utils)\ntmp <- noquote( \n  capture.output( \n    summary(linear.model)\n    )\n  )\nwrite.table(tmp[9:12],quote=F,row.names=F,col.names=F)\n```\n\n<div class='notes'>\n- as part of computing the linear model R does a hypothesis test to determine if the unknown population slope is different than 0\n\n- The second line of the coefficients that begins with NumItems shows\nus\n    - the estimated slope .25924\n    - the standard error of the slope\n    - the test statistic for the test of the slope is 6.98\n    - the $P$-value is nearly 0, so there is strong evidence of a statistically significant linear relationship between the production time and the number of items produced\n</div>\n<!------------------------------------------------------------------>\n\n## Checking for practical significance\n\ncoefficient of determination $R^2$\n\n```{r eval=FALSE}\nsummary(linear.model)\nrsq <- linear.model$r.squared\nrsq.adj <- linear.model$adj.r.squared\n```\n\n```{r, echo=FALSE,warning=FALSE}\n# this is just tricks to trim the output to fit on one slide\nlinear.model <- with( production, lm( Time ~ NumItems ) )\nrequire(utils)\ntmp <- noquote( \n  capture.output( \n    summary(linear.model)\n    )\n  )\nwrite.table(tmp[17],quote=F,row.names=F,col.names=F)\n```\n\n<div class='notes'>\n- $R^2 \\approx$ proportion of total variation in $y$ that is explained by the linear relationship with $x$.  \n- the adjusted $R^2$ is an unbiased estimate of the population coefficient of determination\n- the adjusted R^2 suggests that about 72% of the total variation in production times is explained by the linear relationship between production time an the number of items produced.  This also means that 28% of the variation is unexplained by the linear relationship.\n- in this case the model has strong predicitive power, but a low coefficient of determination might indicate that a model has little practical significance \n</div>\n\n## Confidence Intervals for the Parameters\n\n```{r}\nconfint(linear.model)\n```\n\nWe are 95% confident that the population mean production time increases 0.18 to 0.34 minutes for each additional item produced.\n\n<div class='notes'>\n- Usually we are most interested in the poulation slope, beta_1, whose confidence interval estimate is given on the second line,\n- but if needed the CI for the population intercept, beta_0, is given on the first line\n- the intercept here suggests that 132 to 167 minutes are required if no items are produced ... that hardly seems reasonable, since all of the data is for production runs with 50 to 350 items produced, we shouldn't expect the model to give us meaningful information for 0 items produced ... this is an example of extrapolation\n</div>\n<!------------------------------------------------------------------>\n\n## Confidence Interval for Population Mean Response\n\nAt a production level of 300 items, what is the average production time?\n\n```{r}\nx <- data.frame( NumItems = 300 )\npredict.lm( linear.model, x , interval=\"confidence\")\n```\nWe are 95% confident that, for a production level of 300 items, the average production time is between 217 and 238 minutes.\n\n<div class='notes'>\n</div>\n<!------------------------------------------------------------------>\n\n## Prediction Interval for New Observed Value of Response\n\nAt a production level of 300 items, what is a plausible range of values for the time of a single, new production run?\n\n```{r}\nx <- data.frame( NumItems = 300 )\npredict.lm( linear.model, x , interval=\"prediction\")\n```\n\nWe are 95% confident that, for a production level of 300 items, the production time will be between 192 and 263 minutes.\n\n<div class='notes'>\n- as discussed in your text book, the prediction interval is saying that if we added one more observation to the data set, with the $x$ value of 300, then th e $y$-value will be between 192 and 263. \n- the 95% says that for 95% of samples you'll get a prediction interval that actually contains the next observed response value\n- the prediction interval is often more relevant than the confidence interval because the prediction interval is telling us what $y$-values we can expect to observe at a given level of $x$, whereas the confidence interval tells us the *average* value of $y$ to expect.\n</div>\n<!------------------------------------------------------------------>\n\n## The Lack of Fit Test - An Example\n\n- relationship between drug dose ($x$) and strength of protective response ($y$) (Ott, problem 11.45)\n\n```{r, eval = FALSE}\ndose <- rep(c(2,4,8,16,32,64),c(3,2,2,3,2,3))\nstrength <- c(5,7,3,10,14,15,17,20,21,19,23,29,28,31,30)\ndrug <- data.frame(dose,strength); mod <- lm(strength~dose)\nplot(dose,strength); abline(mod)\n```\n\nSee plot on next slide.\n\n## The Lack of Fit Test \n\n\n<!--\n## Simple Linear Regression Model\n\nSimple Linear Regression: \n$$y_i  = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\hspace{.5in}\n\\epsilon_i \\overset{\\scriptsize\\mbox{ind}}{\\sim} N(0,\\sigma_\\epsilon)$$\n$y$ is the response or dependent variable.\n$x$ is the explanatory or independent variable.\n\n\nANOVA model:\n\n$$ y_{i,j} = \\mu_i + \\epsilon_{i,j},\\hspace{.5in} \\epsilon_{i,j} \\overset{\\scriptsize \\mbox{ind}}{\\sim} N(0,\\sigma_\\epsilon)$$\n\nwhere $y_{i,j}$ is the observation for individual $j$ in group $i$.\n\n<div class='notes'>\nThe simple linear regression model is a reduced or special case of the ANOVA model as we are assuming the group means are related in a particular way.\n</div>\n\n\n\n## The ANOVA Table\n\n\\begin{table}[h!]\n        \\centering\n        \\begin{tabular}{lccccc}\n        Source & df & SS & MS & F  & $P$-value \\\\ \\hline\n         & & & & & \\\\\n        Treatment & $t-1$& $SST$  & $MST=\\frac{SST}{t-1}$ & $F_0 = \\frac{MST}{MSE}$&$P(F_{t-1, n_T-t} > F_0 )$  \\\\[1em]\n        Error & $n_T-t$ & $SSE$ & $MSE = \\frac{SSE}{n_T-t}$ & & \\\\[1em]  \\hline\n        & & & & & \\\\\n        Total & $n_T-1$ & $TSS$  & &  & \n        \\end{tabular}\n\\end{table}\n\n-->",
    "created" : 1439308922114.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2085270658",
    "id" : "62115654",
    "lastKnownWriteTime" : 1439308925,
    "path" : "~/Google Drive/Gdrive_snap_May_15/MS Data Science/DS 705 CEOEL Folder/class_material/lessons/week 09/Week_09_Storybook.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}